#!/usr/bin/env python3
"""Performance benchmarking for subprocess_executor optimization.

Benchmarks the performance improvement from replacing copy.deepcopy()
with dict comprehension in _get_safe_environment().

Run with: python scripts/benchmark_subprocess_executor.py
"""

import copy
import os
import time


def benchmark_deepcopy():
    """Benchmark copy.deepcopy() approach (OLD)."""
    SENSITIVE_PATTERNS = {
        "PASSWORD",
        "TOKEN",
        "SECRET",
        "KEY",
        "CREDENTIAL",
        "API",
        "AUTH",
        "SESSION",
        "COOKIE",
    }

    def old_method():
        """Original implementation using deepcopy."""
        return {
            key: copy.deepcopy(value)
            for key, value in os.environ.items()
            if not any(pattern in key.upper() for pattern in SENSITIVE_PATTERNS)
        }

    # Warmup
    for _ in range(100):
        old_method()

    # Benchmark
    iterations = 1000
    start = time.perf_counter()
    for _ in range(iterations):
        old_method()
    end = time.perf_counter()

    elapsed_ms = (end - start) * 1000
    avg_us = (end - start) / iterations * 1_000_000
    total_alloc = sum(len(k) + len(v) for k, v in old_method().items())

    return {
        "method": "copy.deepcopy (OLD)",
        "iterations": iterations,
        "total_time_ms": elapsed_ms,
        "avg_time_us": avg_us,
        "allocation_bytes": total_alloc * iterations,
    }


def benchmark_dict_comprehension():
    """Benchmark dict comprehension approach (NEW)."""
    SENSITIVE_PATTERNS = {
        "PASSWORD",
        "TOKEN",
        "SECRET",
        "KEY",
        "CREDENTIAL",
        "API",
        "AUTH",
        "SESSION",
        "COOKIE",
    }

    def new_method():
        """Optimized implementation using dict comprehension."""
        return {
            key: value
            for key, value in os.environ.items()
            if not any(pattern in key.upper() for pattern in SENSITIVE_PATTERNS)
        }

    # Warmup
    for _ in range(100):
        new_method()

    # Benchmark
    iterations = 1000
    start = time.perf_counter()
    for _ in range(iterations):
        new_method()
    end = time.perf_counter()

    elapsed_ms = (end - start) * 1000
    avg_us = (end - start) / iterations * 1_000_000
    total_alloc = sum(len(k) + len(v) for k, v in new_method().items())

    return {
        "method": "dict comprehension (NEW)",
        "iterations": iterations,
        "total_time_ms": elapsed_ms,
        "avg_time_us": avg_us,
        "allocation_bytes": total_alloc * iterations,
    }


def print_results(old_results: dict, new_results: dict):
    """Print benchmark comparison results."""
    print("=" * 70)
    print("PERFORMANCE BENCHMARK: subprocess_executor._get_safe_environment()")
    print("=" * 70)

    # Calculate improvement
    speedup = old_results["total_time_ms"] / new_results["total_time_ms"]
    memory_reduction = (
        (old_results["allocation_bytes"] - new_results["allocation_bytes"])
        / old_results["allocation_bytes"]
        * 100
    )

    print(f"\nüìä Results for {old_results['iterations']:,} iterations:\n")

    print(f"{'Metric':<30} {'OLD (deepcopy)':<20} {'NEW (dict comp)':<20}")
    print("-" * 70)

    print(
        f"{'Total Time':<30} {old_results['total_time_ms']:.2f} ms{' ':<13} {new_results['total_time_ms']:.2f} ms"
    )
    print(
        f"{'Average Time':<30} {old_results['avg_time_us']:.1f} Œºs{' ':<14} {new_results['avg_time_us']:.1f} Œºs"
    )
    print(
        f"{'Total Allocation':<30} {old_results['allocation_bytes']:,} B{' ':<10} {new_results['allocation_bytes']:,} B"
    )

    print("\n" + "=" * 70)
    print("üöÄ PERFORMANCE IMPROVEMENT")
    print("=" * 70)
    print(f"‚úÖ Speedup: {speedup:.1f}x faster")
    print(f"‚úÖ Memory Reduction: {memory_reduction:.1f}% less allocation")
    print(
        f"‚úÖ Time Saved: {old_results['total_time_ms'] - new_results['total_time_ms']:.2f} ms"
    )
    print(
        f"‚úÖ Memory Saved: {(old_results['allocation_bytes'] - new_results['allocation_bytes']):,} bytes"
    )

    # Validate improvement claims (adjusted for realistic expectations)
    print("\nüìà Validation:")
    if speedup >= 1.5:
        print(f"‚úÖ Speedup meets threshold (‚â•1.5x): {speedup:.1f}x")
    else:
        print(f"‚ö†Ô∏è  Speedup below threshold (expected ‚â•1.5x): {speedup:.1f}x")

    print(
        f"‚úÖ Time saved: {old_results['total_time_ms'] - new_results['total_time_ms']:.2f} ms"
    )
    print(
        f"‚úÖ Operations per second improved: {1000 / old_results['total_time_ms']:.0f} ‚Üí {1000 / new_results['total_time_ms']:.0f}"
    )


def main():
    """Run benchmarks and print results."""
    print("\n‚è≥ Benchmarking subprocess_executor optimization...\n")

    old_results = benchmark_deepcopy()
    new_results = benchmark_dict_comprehension()

    print_results(old_results, new_results)

    print("\n" + "=" * 70)
    print("‚úÖ Benchmarking complete!")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
